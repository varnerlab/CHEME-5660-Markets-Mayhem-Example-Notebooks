{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8312727f-470f-46ba-bddf-630822894f16",
   "metadata": {},
   "source": [
    "## CHEME 5660 Lab 8: Formulation and Solution of the Branched Tiger Problem as a Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715d45e-ffbe-4d02-bee0-450fb91cb00f",
   "metadata": {},
   "source": [
    "<img src=\"./figs/Fig-Branched-MDP-Schematic-no-a-labels.png\" style=\"margin:auto; width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8870c99-4aae-494f-87a7-e1ce3bd0838c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28aa5b-3634-44a7-a5f9-af0b7a7b9f96",
   "metadata": {},
   "source": [
    "## Lab 8 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368267bf-19ad-4b4f-a86e-bdecbf93f9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\"); Pkg.resolve(); Pkg.instantiate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98508d0f-d5b9-4494-b283-4b2965e38ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load req packages -\n",
    "using PrettyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b140713-5c3b-4a9a-8bd7-5b142655d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"CHEME-5660-Lab-8-CodeLib.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1146fa85-7837-4d82-a871-36fd01da26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some global constants -\n",
    "Î± = 0.80; # probability of moving the direction we are expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21bfa98-1884-4068-99cd-bf0615dc5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the states and actions -\n",
    "safety = 1;\n",
    "tiger = 15;\n",
    "\n",
    "states = range(safety,stop=tiger, step=1) |> collect;\n",
    "actions = [1,2,3,4]; # aâ‚ = move left, aâ‚‚ = move right, aâ‚ƒ = move up, aâ‚„ = move down\n",
    "Î³ = 0.95;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95590ae-79d2-49a4-ad96-cd8d866ab3a5",
   "metadata": {},
   "source": [
    "#### Configure the rewards array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8934d3-ec5d-4264-91c1-e01a7a8e4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the rewards -\n",
    "R = Array{Float64,2}(undef,length(states), length(actions));\n",
    "\n",
    "# most of the rewards are zero -\n",
    "fill!(R,0.0) # fill R w/zeros\n",
    "\n",
    "# set the rewards for the ends -\n",
    "R[safety + 1,1] = 10; # if in state 2, and we take action 1 = we live, get married, our kids are all doctors, and we are generally content\n",
    "R[tiger-1, 2] = -100; # if in state N - 1, and we take action 2 = we get eaten. Bad.\n",
    "\n",
    "# rewards for the by-passes. \n",
    "R[2,3] = -10.0;\n",
    "R[2,4] = -1.0;\n",
    "\n",
    "R[9,3] = -1.0;\n",
    "R[9,4] = -1.0;\n",
    "\n",
    "\n",
    "R[8,1] = 0.0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c110013-4c5d-4759-8646-05ae9ce940b1",
   "metadata": {},
   "source": [
    "#### Configure the transition array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b160d5-3a73-4e29-8b43-a67beba9b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the transitions\n",
    "T = Array{Float64,3}(undef, length(states), length(states), length(actions));\n",
    "fill!(T,0.0);\n",
    "\n",
    "# We need to put values into the transition array (these are probabilities, so eah row much sum to 1)\n",
    "T[safety, 1, 1:length(actions)] .= 1.0; # if we are in state 1, we stay in state 1 âˆ€a âˆˆ ğ’œ\n",
    "T[tiger, tiger, 1:length(actions)] .= 1.0; # if we are in state 5, we stay in state 5 \n",
    "\n",
    "# left actions -\n",
    "for s âˆˆ 2:(tiger - 1)\n",
    "    T[s,s-1,1] = Î±;\n",
    "    T[s,s+1,1] = (1-Î±);\n",
    "end\n",
    "\n",
    "# right actions -\n",
    "for s âˆˆ 2:(tiger - 1)\n",
    "    T[s,s-1,2] = (1-Î±);\n",
    "    T[s,s+1,2] = Î±; \n",
    "end\n",
    "\n",
    "# Node 2 -\n",
    "T[2,:,2] .= 0.0\n",
    "T[2,3,3] = Î±;\n",
    "T[2,6,3] = (1-Î±);\n",
    "T[2,6,4] = Î±;\n",
    "T[2,3,4] = (1-Î±);\n",
    "\n",
    "# Node 3 -\n",
    "T[3,2,4] = Î±;\n",
    "T[3,4,4] = (1-Î±);\n",
    "T[3,2,1] = 0.0;\n",
    "T[3,4,1] = 0.0;\n",
    "\n",
    "# Node 5 -\n",
    "T[5,:,2] .= 0.0;\n",
    "T[5,9,4] = Î±;\n",
    "T[5,4,4] = (1-Î±);\n",
    "\n",
    "# Node 6 -\n",
    "T[6,2,3] = Î±;\n",
    "T[6,7,3] = (1-Î±);\n",
    "\n",
    "# Node 8 -\n",
    "T[8,:,2] .= 0.0\n",
    "T[8,9,3] = Î±;\n",
    "T[8,7,3] = (1-Î±);\n",
    "\n",
    "# Node 9 -\n",
    "T[9,:,1] .= 0.0\n",
    "T[9,8,4] = Î±;\n",
    "T[9,5,4] = (1-Î±);\n",
    "T[9,5,3] = Î±;\n",
    "T[9,8,3] = (1-Î±);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25960165-2f78-4737-8184-d48d0d9fab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_problem = build(MDPProblem; ğ’® = states, ğ’œ = actions, T = T, R = R, Î³ = Î³);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "951c15bb-bc1c-4827-b00e-3aa3a0305e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Vector{Float64}:\n",
       "  0.0\n",
       " 12.127173992305273\n",
       " 11.195652591080394\n",
       " 10.415791352254672\n",
       " 10.037344121229339\n",
       " 11.164961544819933\n",
       " 10.25425952983119\n",
       "  9.309940819831795\n",
       "  7.982650406105737\n",
       "  7.351565491190357\n",
       "  6.7618483292105065\n",
       "  6.182413452135977\n",
       "  5.491624852294697\n",
       "  4.17363488774397\n",
       "  0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = solve(mdp_problem,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09169577-9d50-4ee6-a355-2e015900bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_array = Q(mdp_problem, U)[2:end-1,:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b42f877a-fde0-4dba-b266-9738bd462fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚\u001b[1m State s \u001b[0mâ”‚\u001b[1m Action \u001b[0mâ”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚       2 â”‚   left â”‚\n",
      "â”‚       3 â”‚   down â”‚\n",
      "â”‚       4 â”‚   left â”‚\n",
      "â”‚       5 â”‚   left â”‚\n",
      "â”‚       6 â”‚     up â”‚\n",
      "â”‚       7 â”‚   left â”‚\n",
      "â”‚       8 â”‚   left â”‚\n",
      "â”‚       9 â”‚   down â”‚\n",
      "â”‚      10 â”‚   left â”‚\n",
      "â”‚      11 â”‚   left â”‚\n",
      "â”‚      12 â”‚   left â”‚\n",
      "â”‚      13 â”‚   left â”‚\n",
      "â”‚      14 â”‚   left â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# compute the policy -\n",
    "policy = Ï€(Q_array);\n",
    "\n",
    "# display the policy -\n",
    "policy_data_array = Array{Any,2}(undef, length(states)-2, 2);\n",
    "\n",
    "for s = 1:length(states)-2\n",
    "    \n",
    "    policy_keyword = \"left\"\n",
    "    policy_index = policy[s];\n",
    "    if policy_index == 2\n",
    "       policy_keyword = \"right\" \n",
    "    elseif policy_index == 3\n",
    "        policy_keyword = \"up\" \n",
    "    elseif policy_index == 4\n",
    "        policy_keyword = \"down\" \n",
    "    end\n",
    "    \n",
    "    policy_data_array[s,1] = s+1;\n",
    "    policy_data_array[s,2] = policy_keyword;    \n",
    "end\n",
    "\n",
    "policy_table_header = ([\"State s\", \"Action\"])\n",
    "\n",
    "pretty_table(policy_data_array; header=policy_table_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6c6dd-7599-4605-beff-3491e0ca0459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
