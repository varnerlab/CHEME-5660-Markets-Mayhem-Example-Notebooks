mutable struct MDP

    # data -
    ğ’®::Array{Int64,1}
    ğ’œ::Array{Int64,1}
    T::Array{Float64,3}
    R::Array{Float64,2}
    Î³::Float64
    

    # constructor -
    MDP() = new()
end

function build(type::Type{MDP}; 
    ğ’®::Array{Int64,1} = Array{Int64,1}(undef,1), 
    ğ’œ::Array{Int64,1} = Array{Int64,2}(undef,1), 
    T::Array{Float64,3} = Array{Float64,3}(undef,1,1,1), 
    R::Array{Float64,2} = Array{Float64,2}(undef, 1,1), 
    Î³::Float64 = 0.1)

    # build and empty MDP -
    m = MDP();

    # add data -
    m.R = R;
    m.T = T;
    m.ğ’œ = ğ’œ;
    m.ğ’® = ğ’®;
    m.Î³ = Î³

    # return -
    return m;
end

function lookahead(p::MDP, U::Vector{Float64}, s::Int64, a::Int64)

    # grab stuff from the problem -
    R = p.R;  # reward -
    T = p.T;    
    Î³ = p.Î³;
    ğ’® = p.ğ’®;
    
    # setup my state array -
    return R[s,a] + Î³*sum(T[s,sâ€²,a]*U[i] for (i,sâ€²) in enumerate(ğ’®))
end

function iterative_policy_evaluation(p::MDP, Ï€, k_max)

    # grab stuff from the problem -
    R = p.R;  # reward -
    T = p.T;    
    Î³ = p.Î³;
    ğ’® = p.ğ’®;

    # initialize value -
    U = [0.0 for s âˆˆ ğ’®];

    for k âˆˆ 1:k_max
        U = [lookahead(p, U, s, Ï€(s)) for s âˆˆ ğ’®]
    end

    return U;
end