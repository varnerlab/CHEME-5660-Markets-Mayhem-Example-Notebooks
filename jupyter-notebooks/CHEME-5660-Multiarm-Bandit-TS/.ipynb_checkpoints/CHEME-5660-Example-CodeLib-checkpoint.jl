abstract type AbstractSamplingModel end

mutable struct ThompsonSamplingModel <: AbstractSamplingModel

    # data -
    Î±::Array{Float64,1}
    Î²::Array{Float64,1}
    K::Int64

    # constructor -
    ThompsonSamplingModel() = new();
end

mutable struct EpsilonSamplingModel <: AbstractSamplingModel

    # data -
    Î±::Array{Float64,1}
    Î²::Array{Float64,1}
    K::Int64
    Ïµ::Float64

    # constructor -
    EpsilonSamplingModel() = new();
end

# placeholder - always return 0
_null(action::Int64)::Int64 = return 0;


function sample(model::EpsilonSamplingModel;  ð’¯::Int64 = 0, world::Function = _null)::Dict{Int64,Beta}

    # initialize -
    Î± = model.Î±
    Î² = model.Î²
    K = model.K
    Ïµ = model.Ïµ
    Î¸Ì‚_vector = Array{Float64,1}(undef, K)

    # generate random Categorical distribution -
    dcat = Categorical([0.30, 0.35, 0.35]);

    # initialize collection of Beta distributions -
    action_distribution_dict = Dict{Int64, Beta}();
    for k âˆˆ 1:K
        action_distribution_dict[k] = Beta(Î±[k], Î²[k]); # initialize uniform
    end

    # main sampling loop -
    for _ âˆˆ 1:ð’¯
    
        aâ‚œ = 1; # default to 1
        if (rand() < Ïµ)
            aâ‚œ = rand(dcat);
        else
            
            for k âˆˆ 1:K

                # grab the distribution for action k -
                d = action_distribution_dict[k];
    
                # generate a sample for this action -
                Î¸Ì‚_vector[k] = rand(d);
            end

            # ok: let's choose an action -
            aâ‚œ = argmax(Î¸Ì‚_vector);
        end

        # pass that action to the world function, gives back a reward -
        râ‚œ = world(aâ‚œ);

        # update the parameters -
        # first, get the old parameters -
        old_d = action_distribution_dict[aâ‚œ];
        Î±,Î² = params(old_d);

        # update the old values with the new values -
        Î± = Î± + râ‚œ
        Î² = Î² + (1-râ‚œ)

        # build new distribution -
        action_distribution_dict[aâ‚œ] = Beta(Î±, Î²);
    end

    # return -
    return action_distribution_dict
end

# main sampling method -
function sample(model::ThompsonSamplingModel; ð’¯::Int64 = 0, world::Function = _null)::Dict{Int64,Beta}

    # initialize -
    Î± = model.Î±
    Î² = model.Î²
    K = model.K
    Î¸Ì‚_vector = Array{Float64,1}(undef, K)

    # initialize collection of Beta distributions -
    action_distribution_dict = Dict{Int64, Beta}();
    for k âˆˆ 1:K
        action_distribution_dict[k] = Beta(Î±[k], Î²[k]); # initialize uniform
    end

    # main sampling loop -
    for _ âˆˆ 1:ð’¯
        for k âˆˆ 1:K

            # grab the distribution for action k -
            d = action_distribution_dict[k];

            # generate a sample for this action -
            Î¸Ì‚_vector[k] = rand(d);
        end

        # ok: let's choose an action -
        aâ‚œ = argmax(Î¸Ì‚_vector);

        # pass that action to the world function, gives back a reward -
        râ‚œ = world(aâ‚œ);

        # update the parameters -
        # first, get the old parameters -
        old_d = action_distribution_dict[aâ‚œ];
        Î±,Î² = params(old_d);

        # update the old values with the new values -
        Î± = Î± + râ‚œ
        Î² = Î² + (1-râ‚œ)

        # build new distribution -
        action_distribution_dict[aâ‚œ] = Beta(Î±, Î²);
    end
    
    # return -
    return action_distribution_dict;
end 