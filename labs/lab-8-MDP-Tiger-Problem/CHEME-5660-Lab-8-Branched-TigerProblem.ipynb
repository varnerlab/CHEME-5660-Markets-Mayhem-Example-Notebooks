{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8312727f-470f-46ba-bddf-630822894f16",
   "metadata": {},
   "source": [
    "## CHEME 5660 Lab 8: Markov Decision Process (MDP) solution of the Branched Tiger Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715d45e-ffbe-4d02-bee0-450fb91cb00f",
   "metadata": {},
   "source": [
    "<img src=\"./figs/Fig-Branched-MDP-Schematic-no-a-labels.png\" style=\"margin:auto; width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcfedf-9f68-483f-a243-5139901cc463",
   "metadata": {},
   "source": [
    "__Fig. 1__: Schematic of the Tiger problem modeled as an N-state, four-action (left, right, up, down) Markov decision process. The hallway has three types of paths: unobstructed (white, free), mildly obstructed (light gray, small cost), and obstructed (dark gray, large cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8870c99-4aae-494f-87a7-e1ce3bd0838c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "A Markov decision process is the tuple $\\left(\\mathcal{S}, \\mathcal{A}, R_{a}\\left(s, s^{\\prime}\\right), T_{a}\\left(s,s^{\\prime}\\right), \\gamma\\right)$ where:\n",
    "\n",
    "* The state space $\\mathcal{S}$ is the set of all possible states $s$ that a system can exist in\n",
    "* The action space $\\mathcal{A}$ is the set of all possible actions $a$ that are available to the agent, where $\\mathcal{A}_{s} \\subseteq \\mathcal{A}$ is the subset of the action space $\\mathcal{A}$ that is accessible from state $s$.\n",
    "* An expected immediate reward $R_{a}\\left(s, s^{\\prime}\\right)$ is received after transitioning from state $s\\rightarrow{s}^{\\prime}$ due to action $a$. \n",
    "* The transition $T_{a}\\left(s,s^{\\prime}\\right) = P(s_{t+1} = s^{\\prime}~|~s_{t}=s,a_{t} = a)$ denotes the probability that action $a$ in state $s$ at time $t$ will result in state $s^{\\prime}$ at time $t+1$\n",
    "* The quantity $\\gamma$ is a _discount factor_; the discount factor is used to weigh the _future expected utility_.\n",
    "\n",
    "Finally, a policy function $\\pi$ is the (potentially probabilistic) mapping from states $s\\in\\mathcal{S}$ to actions $a\\in\\mathcal{A}$ used by the agent to solve the decision task. \n",
    "\n",
    "### Value iteration\n",
    "In the previous example, we saw how we could develop _a policy_ $\\pi(s)$ by looking at the values in the $Q$-array. However, this required the utility vector; thus, we needed to hypothesize a policy that may not be the _optimal policy_. There are two techniques to compute optimal policies, and we'll explore the simpler of the two, namely _value iteration_. In _value iteration_, the value function (the vector of utility values) is updated iteratively using the _Bellman update__ procedure:\n",
    "\n",
    "$$\n",
    "U_{k+1}(s) = \\max_{a}\\left(R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s^{\\prime} | s, a)U_{k}(s^{\\prime})\\right)\n",
    "$$\n",
    "\n",
    "This procedure is guaranteed to converge to the optimal utility vector (value function).  \n",
    "\n",
    "### Problem\n",
    "\n",
    "An agent is trapped in a long hallway with two doors at either end (Fig. 1). Behind the green door is a tiger (and certain death), while behind the red door is freedom. If the agent opens the green door, the agent is eaten (and receives a large negative reward). However, if the agent opens the red door, it escapes and gets a positive reward. \n",
    "\n",
    "For this problem, the MDP has the tuple components:\n",
    "* $\\mathcal{S} = \\left\\{1,2,\\dots,N\\right\\}$ while the action set is $\\mathcal{A} = \\left\\{a_{1},a_{2}, a_{3}, a_{4}\\right\\}$; action $a_{1}$ moves the agent one state to the left, action $a_{2}$ moves the agent one state to the right, action $a_{3}$ moves the agent one stop up, and action $a_{4}$ moves the agent one step down.\n",
    "* The agent receives a postive reward for entering the red state $N$ (escapes). However, the agent is penalized for entering the green state $1$ (eaten by the tiger).  Finally, the agent is not charged to move to adjacent locations if those locations are unobstructed. However, there is a small charge to move through mildly obstructed locations (light gray circles) and a larger charge to move through obstructed areas (dark gray circles).\n",
    "* Let the probability of correctly executing an action $a_{j}\\in\\mathcal{A}$ be $\\alpha$.\n",
    "\n",
    "Let's use value iteration to estimate the _optimal policy_ $\\pi^{\\star}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28aa5b-3634-44a7-a5f9-af0b7a7b9f96",
   "metadata": {},
   "source": [
    "## Lab 8 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368267bf-19ad-4b4f-a86e-bdecbf93f9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5660-Markets-Mayhem-Example-Notebooks/labs/lab-8-MDP-Tiger-Problem/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\"); Pkg.resolve(); Pkg.instantiate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98508d0f-d5b9-4494-b283-4b2965e38ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load req packages -\n",
    "using PrettyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b140713-5c3b-4a9a-8bd7-5b142655d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"CHEME-5660-Lab-8-CodeLib.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417dfccb-ae54-49b8-be93-587dceaeadf8",
   "metadata": {},
   "source": [
    "#### Configure constants, states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a21bfa98-1884-4068-99cd-bf0615dc5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some global constants -\n",
    "Î± = 0.95; # probability of moving the direction we are expect\n",
    "\n",
    "# setup the states and actions -\n",
    "safety = 1;\n",
    "tiger = 15;\n",
    "left_reward = -100.0;\n",
    "right_reward = 1000.0;\n",
    "\n",
    "# by pass costs -\n",
    "top_hallway_cost = -20.0;\n",
    "bottom_hallway_cost = -1.0;\n",
    "blocked = -10000.0;\n",
    "\n",
    "states = range(safety,stop=tiger, step=1) |> collect;\n",
    "actions = [1,2,3,4]; # aâ‚ = move left, aâ‚‚ = move right, aâ‚ƒ = move up, aâ‚„ = move down\n",
    "Î³ = 0.95; # how much do we consider future moves?\n",
    "\n",
    "# scenario and setup flags -\n",
    "bottom_hallway_cave_in_flag = false;\n",
    "should_run_T_check = false;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95590ae-79d2-49a4-ad96-cd8d866ab3a5",
   "metadata": {},
   "source": [
    "#### Configure the rewards array $R(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8934d3-ec5d-4264-91c1-e01a7a8e4fb5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the rewards -\n",
    "R = Array{Float64,2}(undef,length(states), length(actions));\n",
    "\n",
    "# most of the rewards are zero -\n",
    "fill!(R,0.0) # fill R w/zeros\n",
    "\n",
    "# set the rewards for the ends -\n",
    "R[safety + 1, 1] = left_reward; # if in state 2, and we take action 1. Bad.\n",
    "R[tiger - 1, 2] = right_reward; # if in state N - 1, and we take action 2, we live, our kids are all doctors, and we are generally content\n",
    "\n",
    "# linear nodes are blocked -\n",
    "list_linear_nodes = range(10,stop=(tiger-1),step = 1) |> collect\n",
    "for i âˆˆ list_linear_nodes\n",
    "    R[i,3] = blocked;\n",
    "    R[i,4] = blocked;\n",
    "end\n",
    "\n",
    "# actions:\n",
    "# aâ‚ = left\n",
    "# aâ‚‚ = right\n",
    "# aâ‚ƒ = up\n",
    "# aâ‚„ = down\n",
    "\n",
    "# rewards for the by-passes. \n",
    "R[2,3] = top_hallway_cost;\n",
    "R[2,4] = bottom_hallway_cost;\n",
    "R[2,2] = blocked;\n",
    "R[9,1] = blocked;\n",
    "R[9,3] = top_hallway_cost;\n",
    "R[9,4] = bottom_hallway_cost;\n",
    "\n",
    "# top -\n",
    "R[3,1] = blocked;\n",
    "R[3,2] = top_hallway_cost\n",
    "R[3,3] = blocked;\n",
    "R[4,1] = top_hallway_cost\n",
    "R[4,2] = top_hallway_cost\n",
    "R[4,3] = blocked;\n",
    "R[4,4] = blocked;\n",
    "R[5,1] = top_hallway_cost\n",
    "R[5,2] = blocked;\n",
    "R[5,3] = blocked;\n",
    "\n",
    "# bottom -\n",
    "R[6,1] = blocked;\n",
    "R[6,2] = bottom_hallway_cost;\n",
    "R[6,4] = blocked;\n",
    "R[7,1] = bottom_hallway_cost;\n",
    "R[7,2] = bottom_hallway_cost;\n",
    "R[7,3] = blocked;\n",
    "R[7,4] = blocked;\n",
    "R[8,1] = bottom_hallway_cost;\n",
    "R[8,2] = blocked;\n",
    "R[8,4] = blocked;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c110013-4c5d-4759-8646-05ae9ce940b1",
   "metadata": {},
   "source": [
    "#### Configure the transition array $T_{a}(s,s^{\\prime})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b160d5-3a73-4e29-8b43-a67beba9b798",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the transitions\n",
    "T = Array{Float64,3}(undef, length(states), length(states), length(actions));\n",
    "fill!(T,0.0);\n",
    "\n",
    "# We need to put values into the transition array (these are probabilities, so eah row much sum to 1)\n",
    "T[safety, 1, 1:length(actions)] .= 1.0; # if we are in state 1, we stay in state 1 âˆ€a âˆˆ ğ’œ\n",
    "T[tiger, tiger, 1:length(actions)] .= 1.0; # if we are in state 5, we stay in state 5 \n",
    "\n",
    "# left actions -\n",
    "for s âˆˆ 2:(tiger - 1)\n",
    "    T[s,s-1,1] = Î±;\n",
    "    T[s,s+1,1] = (1-Î±);\n",
    "end\n",
    "\n",
    "# right actions -\n",
    "for s âˆˆ 2:(tiger - 1)\n",
    "    T[s,s-1,2] = (1-Î±);\n",
    "    T[s,s+1,2] = Î±; \n",
    "end\n",
    "\n",
    "# Node 2 -\n",
    "T[2,:,2] .= 0.0\n",
    "T[2,3,3] = Î±;\n",
    "T[2,6,3] = (1-Î±);\n",
    "T[2,6,4] = Î±;\n",
    "T[2,3,4] = (1-Î±);\n",
    "T[2,2,2] = 1.0; # node 2 can't go right\n",
    "\n",
    "# Node 3 -\n",
    "T[3,:,:] .= 0.0\n",
    "T[3,3,1] = 1.0;\n",
    "T[3,4,2] = Î±;\n",
    "T[3,3,2] = (1-Î±);\n",
    "T[3,3,3] = 1.0;\n",
    "T[3,2,4] = Î±;\n",
    "T[3,3,4] = (1-Î±);\n",
    "\n",
    "# Node 4 -\n",
    "T[4,4,3] = 1.0;\n",
    "T[4,4,4] = 1.0;\n",
    "\n",
    "# Node 5 -\n",
    "T[5,:,:] .= 0.0;\n",
    "T[5,4,1] = Î±;\n",
    "T[5,5,1] = (1-Î±);\n",
    "T[5,5,2] = 1.0;\n",
    "T[5,5,3] = 1.0;\n",
    "T[5,9,4] = Î±;\n",
    "T[5,5,4] = (1-Î±);\n",
    "\n",
    "# Node 6 -\n",
    "T[6,:,:] .= 0.0;\n",
    "T[6,6,1] = 1.0;\n",
    "T[6,7,2] = Î±;\n",
    "T[6,6,2] = (1-Î±);\n",
    "T[6,2,3] = Î±;\n",
    "T[6,6,3] = (1-Î±);\n",
    "T[6,6,4] = 1.0;\n",
    "\n",
    "# Node 7 -\n",
    "T[7,7,3] = 1.0;\n",
    "T[7,7,4] = 1.0;\n",
    "\n",
    "# Node 8 -\n",
    "T[8,:,:] .= 0.0\n",
    "T[8,7,1] = Î±;\n",
    "T[8,8,1] = (1-Î±);\n",
    "T[8,8,2] = 1.0;\n",
    "T[8,9,3] = Î±;\n",
    "T[8,8,3] = (1-Î±);\n",
    "T[8,8,4] = 1.0;\n",
    "\n",
    "# Node 9 -\n",
    "T[9,:,:] .= 0.0;\n",
    "T[9,9,1] = 1.0;\n",
    "T[9,10,2] = Î±;\n",
    "T[9,9,2] = (1-Î±);\n",
    "T[9,5,3] = Î±;\n",
    "T[9,9,3] = (1-Î±);\n",
    "T[9,8,4] = Î±;\n",
    "T[9,9,4] = (1-Î±);\n",
    "\n",
    "# Nodes 10 -> end\n",
    "for s = 10:(tiger-1)\n",
    "    T[s,s,3] = 1.0\n",
    "    T[s,s,4] = 1.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7092317c-adb8-49ff-9724-77f991c406b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-configure setup to include bttom hallway cave in\n",
    "if (bottom_hallway_cave_in_flag == true)\n",
    "    \n",
    "    # configure rewards -\n",
    "    R[6,2] = blocked;\n",
    "    R[8,1] = blocked;\n",
    "\n",
    "    # configure T array -\n",
    "    # node 6 -\n",
    "    T[6,:,:] .= 0.0;\n",
    "    T[6,6,1] = 1.0; # we stay in 6 if we go left\n",
    "    T[6,6,2] = 1.0; # we stay in 6 if we go right\n",
    "    T[6,2,3] = Î±; \n",
    "    T[6,6,3] = (1-Î±);\n",
    "    T[6,6,4] = 1.0; # we stay in 6 if we go down\n",
    "    \n",
    "    # node 8 -\n",
    "    T[8,:,:] .= 0.0;\n",
    "    T[8,9,3] = Î±; \n",
    "    T[8,8,3] = (1 - Î±); \n",
    "    T[8,8,1] = 1.0; # we stay in 8 if we go left\n",
    "    T[8,8,2] = 1.0; # we stay in 8 if we go right\n",
    "    T[8,8,4] = 1.0; # we stay in 8 if we go down\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1781bae-e6d9-4278-a565-46614de13252",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (should_run_T_check == true)\n",
    "    # row summation check -\n",
    "    T_array_check_table = Array{Any,2}(undef, length(states), length(actions)+1)\n",
    "\n",
    "    for s âˆˆ 1:length(states)\n",
    "        T_array_check_table[s,1] = s;\n",
    "    end\n",
    "\n",
    "    for a âˆˆ 1:length(actions)\n",
    "\n",
    "        # sum the action table -\n",
    "        Z = sum(T[:,:,a], dims=2)\n",
    "\n",
    "        for s âˆˆ 1:length(states)\n",
    "            T_array_check_table[s,a+1] = Z[s]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # header -\n",
    "    T_check_header = ([\"State s\", \"aâ‚ (left)\", \"aâ‚‚ (right)\", \"aâ‚ƒ (up)\", \"aâ‚„ (down)\"]);\n",
    "\n",
    "    # display -\n",
    "    pretty_table(T_array_check_table; header=T_check_header)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2bcb6-9053-49b9-8837-bd37aab9affe",
   "metadata": {},
   "source": [
    "### Build the MDP problem object and estimate $\\pi(s)^{\\star}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25960165-2f78-4737-8184-d48d0d9fab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_problem = build(MDPProblem; ğ’® = states, ğ’œ = actions, T = T, R = R, Î³ = Î³);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "951c15bb-bc1c-4827-b00e-3aa3a0305e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = solve(mdp_problem,1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09169577-9d50-4ee6-a355-2e015900bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Q array -\n",
    "Q_array = Q(mdp_problem, U)\n",
    "\n",
    "# compute the policy -\n",
    "policy = Ï€(Q_array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d546b2f-4973-4841-97ca-b8a7e019c20d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚\u001b[1m State \u001b[0mâ”‚\u001b[1m Direction \u001b[0mâ”‚\u001b[1m Ï€(s) \u001b[0mâ”‚\u001b[1m U(aâ‚) (left) \u001b[0mâ”‚\u001b[1m U(aâ‚‚) (right) \u001b[0mâ”‚\u001b[1m U(aâ‚ƒ) (up) \u001b[0mâ”‚\u001b[1m U(aâ‚„) (down) \u001b[0mâ”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚     1 â”‚      stop â”‚    0 â”‚          0.0 â”‚           0.0 â”‚        0.0 â”‚          0.0 â”‚\n",
      "â”‚     2 â”‚      down â”‚    4 â”‚     -70.1118 â”‚       -9399.0 â”‚    579.652 â”‚      632.629 â”‚\n",
      "â”‚     3 â”‚     right â”‚    2 â”‚     -9402.24 â”‚       629.226 â”‚   -9402.24 â”‚      600.836 â”‚\n",
      "â”‚     4 â”‚     right â”‚    2 â”‚      583.474 â”‚       686.247 â”‚   -9348.07 â”‚     -9348.07 â”‚\n",
      "â”‚     5 â”‚      down â”‚    4 â”‚      634.936 â”‚      -9288.04 â”‚   -9288.04 â”‚      749.428 â”‚\n",
      "â”‚     6 â”‚     right â”‚    2 â”‚     -9364.48 â”‚       668.965 â”‚    602.723 â”‚     -9364.48 â”‚\n",
      "â”‚     7 â”‚     right â”‚    2 â”‚      638.338 â”‚       707.134 â”‚   -9328.22 â”‚     -9328.22 â”‚\n",
      "â”‚     8 â”‚        up â”‚    3 â”‚      672.787 â”‚      -9288.04 â”‚    749.428 â”‚     -9288.04 â”‚\n",
      "â”‚     9 â”‚     right â”‚    2 â”‚      -9248.6 â”‚       790.947 â”‚    693.929 â”‚      712.929 â”‚\n",
      "â”‚    10 â”‚     right â”‚    2 â”‚      755.788 â”‚       834.767 â”‚   -9206.97 â”‚     -9206.97 â”‚\n",
      "â”‚    11 â”‚     right â”‚    2 â”‚      797.781 â”‚       883.321 â”‚   -9160.85 â”‚     -9160.85 â”‚\n",
      "â”‚    12 â”‚     right â”‚    2 â”‚       844.19 â”‚       934.814 â”‚   -9111.93 â”‚     -9111.93 â”‚\n",
      "â”‚    13 â”‚     right â”‚    2 â”‚      893.402 â”‚       989.314 â”‚   -9060.15 â”‚     -9060.15 â”‚\n",
      "â”‚    14 â”‚     right â”‚    2 â”‚      892.856 â”‚       1046.99 â”‚   -9005.36 â”‚     -9005.36 â”‚\n",
      "â”‚    15 â”‚      stop â”‚    0 â”‚          0.0 â”‚           0.0 â”‚        0.0 â”‚          0.0 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# make a Q-table -\n",
    "Q_table_data_array = Array{Any,2}(undef, length(states), length(actions)+3)\n",
    "\n",
    "for s âˆˆ 1:length(states)\n",
    "    \n",
    "    Q_table_data_array[s,1] = s;\n",
    "    \n",
    "    direction = \"left\"\n",
    "    policy_index = policy[s];\n",
    "    if policy_index == 2\n",
    "        direction = \"right\" \n",
    "    elseif policy_index == 3\n",
    "         direction = \"up\" \n",
    "    elseif policy_index == 4\n",
    "         direction = \"down\" \n",
    "    elseif policy_index == 0\n",
    "        direction = \"stop\"\n",
    "    end\n",
    "    \n",
    "    Q_table_data_array[s,2] = direction;\n",
    "    Q_table_data_array[s,3] = policy_index;\n",
    "    \n",
    "    \n",
    "    for a âˆˆ 1:length(actions)\n",
    "        Q_table_data_array[s,a+3] = Q_array[s,a];\n",
    "    end\n",
    "end\n",
    "\n",
    "# header -\n",
    "Q_table_header = ([\"State\", \"Direction\", \"Ï€(s)\", \"U(aâ‚) (left)\", \"U(aâ‚‚) (right)\", \"U(aâ‚ƒ) (up)\", \"U(aâ‚„) (down)\"])\n",
    "\n",
    "# show -\n",
    "pretty_table(Q_table_data_array; header = Q_table_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1522c-e450-4540-9721-78f6a5e0fdbc",
   "metadata": {},
   "source": [
    "<img src=\"./figs/Fig-Branched-MDP-Schematic-no-a-labels.png\" style=\"width:60%; align:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15a0ed-03e9-49c5-9e27-40ee89b94496",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "* [Chapter 7: Mykel J. Kochenderfer, Tim A. Wheeler, Kyle H. Wray \"Algorithms for Decision Making\", MIT Press 2022](https://algorithmsbook.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131fb2d-03e8-4a6b-a3be-98233f06f7d1",
   "metadata": {},
   "source": [
    "### Disclaimer and Risks\n",
    "__This content is offered solely for training and  informational purposes__. No offer or solicitation to buy or sell securities or derivative products, or any investment or trading advice or strategy,  is made, given, or endorsed by the teaching team. \n",
    "\n",
    "__Trading involves risk__. Carefully review your financial situation before investing in securities, futures contracts, options, or commodity interests. Past performance, whether actual or indicated by historical tests of strategies, is no guarantee of future performance or success. Trading is generally inappropriate for someone with limited resources, investment or trading experience, or a low-risk tolerance.  Only risk capital that is not required for living expenses.\n",
    "\n",
    "__You are fully responsible for any investment or trading decisions you make__. Such decisions should be based solely on your evaluation of your financial circumstances, investment or trading objectives, risk tolerance, and liquidity needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
